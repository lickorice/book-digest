## 3.1 Data Parallelism
- ![[Pasted image 20230215020607.png]]
## 3.2 CUDA Program Structure
- Reflects the coexistence of a *host* (CPU) and one or more *devices* (GPUs).
	- Each CUDA file can have a mixture of both host and device code
	- One can think that normal C programs are CUDA programs that contain only *host code*.
- Data-parallel functions are called **kernels**.
![[Pasted image 20230215020900.png]]
- When a kernel function is called or *launched*, it is executed by a large number of threads on a device.
		- All the threads generated are collectively called a *grid*
		- Fig. 3.3 shows the execution of two grids of threads.
			- Note that CPU and GPU execution in this figure do not overlap, but in reality, many applications manage overlapped execution.
		- When all threads of a kernel complete execution, the grid terminates, and the execution continues on the host
## 3.3 A Vector Addition Kernel
![[Pasted image 20230215021324.png]]
- Fig 3.4 shows vector addition in regular C. Note that its time complexity is linear.
![[Pasted image 20230215021414.png]]
- Fig 3.5 shows an outline of the host code that offloads the computation to a device, using a kernel function.
## 3.4 Device Global Memory and Data Transfer
- In CUDA, CPU and GPU (host and device, respectively) have separate memory spaces.
	- That's why GPUs have their own DRAM!
	- E.g. NVIDIA GTX480 comes with 4 GB DRAM, a.k.a. **global memory**, a.k.a. **device memory**.
- Basically, CUDA provides an API to make this process easier
![[Pasted image 20230215022109.png]]
- As shown in Fig. 3.7:
	- `cudaMalloc` allocates a piece of global memory for an object.
	- First parameter: cast to `void**` because the function expects a generic pointer.
- E.g.:
  ![[Pasted image 20230215022524.png]]
	- `d_A` is a float pointer, then its reference is cast to `void**` in `cudaMalloc`.
	- `d_A` should **not** be dereferenced in the host code, and should only be mostly used in kernel functions (and API functions)
- To transfer data, we can simply call `cudaMemcpy`.
![[Pasted image 20230215210135.png]]
	- First parameter is the pointer for the destination;
	- second points to the source;
	- third specifies the number of **bytes** to be copied;
	- last indicates the **types of memory involved in the copy**
		- host-to-host; host-to-device... etc.
![[Pasted image 20230215210417.png]]

## 3.5 Kernel Functions and Threading
- CUDA programming is an instance of the SPMD parallel programming style.
- When a kernel is launched by the host, CUDA runtime generates a **grid of threads** that are organized in a two-level hierarchy.
	- Each grid is organized into an array of thread blocks
	- All blocks of a grid are of the same size, where each block can contain up to 1024 threads (may be outdated)
		![[Pasted image 20230215211515.png]]
	- Fig 3.10 above shows an example where each block consists of 256 threads.
	- The number of threads in each block is **specified by the host code** when a kernel is launched.
	- For a given grid of threads, the number of threads in a block is available in the `blockDim` variable.
	- In general, the dimensions of thread blocks should be **multiples of 32 due to hardware efficiency reasons.** This will be revisited later (warp)
	- Each thread in a block has a unique `threadIdx` value.
		- This allows us to combine `threadIdx` and `blockIdx` along with `blockDim.x` to generate a unique index for each block.
![[Pasted image 20230215212303.png]]
![[Pasted image 20230215212314.png]]
- Fig 3.11 above shows a kernel function for our vector addition example.
- Notice the `__global__` keyword. This indicates that the function is a kernel, and **can only be called from the host function to launch the kernel**.
- One can use **both** `__host__` and `__device__` in a function declaration. This tells the compiler to generate two sets of object files for the same function.
- `i` in Fig. 3.11 is an **automatic (local) variable** and cannot be accessed by other threads.
- Note that there is an `if (i<n)` statement:
	- This is due to the fact that not all vector lengths can be expressed as **multiples of the block size**.
		- E.g., assume vector length 100 with block dimension 32. We can launch four thread blocks, but we would have 128 threads, 28 of which will not do any work.
![[Pasted image 20230215221152.png]]
- Kernels are called via execution configuration parameters (see CUDA C++ guide, also in the digest) which is in between the `<<<>>>` triple brackets, as shown in Fig. 3.14
- Note that `2560` in Fig. 3.14 should be `256.0`.
- This vector addition sample is used for its simplicity, in reality, the overhead of memory allocation, data transfer between host and device, and deallocation will likely make the code slower than the original sequential code.
	- Real applications typically have kernels where much more work is needed relative to the amount of data processed, which makes this overhead worthwhile.
	- Data is also tended to be kept in device memory across multiple kernel calls so that the overhead is amortized.