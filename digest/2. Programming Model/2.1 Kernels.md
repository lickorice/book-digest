- CUDA C++ extends C++ by allowing the programmer to define custom C++ functions called **kernels**.
	- When called, these kernels are executed N times in parallel by N different CUDA threads, as opposed to only once in a single thread like a regular C++ function.
	- Defined using a `__global__` declaration specifier. See [[B.1.1 `__global__`]] for more details.
	- Each thread that executes the kernel is given a unique *thread ID* that is accessible within the kernel through built-in variables 
- Example: adds two vectors A and B of size N and stores the result into vector C. Note that indices of the vectors are accessed via thread IDs (through `threadIdx`):
	```cpp
	// Kernel definition
	__global__ void VecAdd(float* A, float* B, float* C) {
		int i = threadIdx.x;
		C[i] = A[i] + B[i];
	}

	int main() {
		// Kernel invocation with N threads
		VecAdd<<<1, N>>>(A, B, C);
	}
```
